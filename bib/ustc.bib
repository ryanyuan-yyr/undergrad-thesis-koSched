@inproceedings{7939053,
  author    = {Otterness, Nathan and Yang, Ming and Rust, Sarah and Park, Eunbyung and Anderson, James H. and Smith, F. Donelson and Berg, Alex and Wang, Shige},
  booktitle = {{2017 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)}},
  title     = {{An Evaluation of the NVIDIA TX1 for Supporting Real-Time Computer-Vision Workloads}},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {353-364},
  doi       = {10.1109/RTAS.2017.3}
}

@inproceedings{8277284,
  author    = {Amert, Tanya and Otterness, Nathan and Yang, Ming and Anderson, James H. and Smith, F. Donelson},
  booktitle = {{2017 IEEE Real-Time Systems Symposium (RTSS)}},
  title     = {{GPU Scheduling on the NVIDIA TX2: Hidden Details Revealed}},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {104-115},
  doi       = {10.1109/RTSS.2017.00017}
}

@inproceedings{DBLP:conf/ecrts/YangOABAS18,
  author    = {Ming Yang and
               Nathan Otterness and
               Tanya Amert and
               Joshua Bakita and
               James H. Anderson and
               F. Donelson Smith},
  editor    = {Sebastian Altmeyer},
  title     = {{Avoiding Pitfalls when Using {NVIDIA}} GPUs for Real-Time Tasks in
               Autonomous Systems},
  booktitle = {{30th Euromicro Conference on Real-Time Systems, {ECRTS}} 2018, July
               3-6, 2018, Barcelona, Spain},
  series    = {LIPIcs},
  volume    = {106},
  pages     = {20:1--20:21},
  publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
  year      = {2018},
  url       = {https://doi.org/10.4230/LIPIcs.ECRTS.2018.20},
  doi       = {10.4230/LIPIcs.ECRTS.2018.20},
  timestamp = {Wed, 04 May 2022 14:28:54 +0200},
  biburl    = {https://dblp.org/rec/conf/ecrts/YangOABAS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{9113104,
  author    = {Olmedo, Ignacio Sañudo and Capodieci, Nicola and Martinez, Jorge Luis and Marongiu, Andrea and Bertogna, Marko},
  booktitle = {{2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)}},
  title     = {{Dissecting the CUDA scheduling hierarchy: a Performance and Predictability Perspective}},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {213-225},
  doi       = {10.1109/RTAS48715.2020.000-5}
}

@article{8853389,
  author  = {Shekofteh, S.-Kazem and Noori, Hamid and Naghibzadeh, Mahmoud and Fröning, Holger and Yazdi, Hadi Sadoghi},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  title   = {{cCUDA: Effective Co-Scheduling of Concurrent Kernels on GPUs}},
  year    = {2020},
  volume  = {31},
  number  = {4},
  pages   = {766-778},
  doi     = {10.1109/TPDS.2019.2944602}
}

@inproceedings{otterness2017inferring,
  title     = {{Inferring the Scheduling Policies of an Embedded {CUDA} {GPU}}},
  author    = {Otterness, Nathan and Yang, Ming and Amert, Tanya and Anderson, James H. and Smith, F. D.},
  booktitle = {{Workshop on Operating Systems Platforms for Embedded Real-Time Applications (OSPERT)}},
  year      = {2017}
}

@online{nsightsystems,
  author  = {NVIDIA},
  title   = {{NVIDIA Developer}},
  year    = 2023,
  url     = {https://developer.nvidia.com/nsight-systems},
  urldate = {2023-03-20}
}

@article{10.1145_3295690,
  author     = {Shekofteh, S.-Kazem and Noori, Hamid and Naghibzadeh, Mahmoud and Yazdi, Hadi Sadoghi and Fr\"{o}ning, Holger},
  title      = {{Metric Selection for GPU Kernel Classification}},
  year       = {2019},
  issue_date = {December 2018},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {15},
  number     = {4},
  issn       = {1544-3566},
  url        = {https://doi.org/10.1145/3295690},
  doi        = {10.1145/3295690},
  journal    = {ACM Trans. Archit. Code Optim.},
  month      = {jan},
  articleno  = {68},
  numpages   = {27},
  keywords   = {resource utilization, kernel metrics, Classification, feature selection, concurrency}
}

@misc{cuda_samples,
  author = {NVIDIA},
  title  = {{CUDA Samples SDK Reference Manual v7.0}},
  year   = {2015}
}

@article{10.1145/1498765.1498785,
author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
title = {{Roofline: An Insightful Visual Performance Model for Multicore Architectures}},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/1498765.1498785},
doi = {10.1145/1498765.1498785},
journal = {Commun. ACM},
month = {apr},
pages = {65–76},
numpages = {12}
}

@article{KONSTANTINIDIS201737,
title = {{A quantitative roofline model for GPU kernel performance estimation using micro-benchmarks and hardware metric profiling}},
journal = {Journal of Parallel and Distributed Computing},
volume = {107},
pages = {37-56},
year = {2017},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517301247},
author = {Elias Konstantinidis and Yiannis Cotronis},
keywords = {Performance estimation, Performance model, Micro-benchmarks, GPU computing, CUDA, HIP},
}

@inproceedings{10.5555/1413370.1413402,
author = {Volkov, Vasily and Demmel, James W.},
title = {{Benchmarking GPUs to Tune Dense Linear Algebra}},
year = {2008},
isbn = {9781424428359},
publisher = {IEEE Press},
booktitle = {{Proceedings of the 2008 ACM/IEEE Conference on Supercomputing}},
articleno = {31},
numpages = {11},
location = {Austin, Texas},
series = {SC '08}
}

@inproceedings{10.1145/2063384.2063392,
  author    = {Nath, Rajib and Tomov, Stanimire and Dong, Tingxing "Tim" and Dongarra, Jack},
  title     = {{Optimizing Symmetric Dense Matrix-Vector Multiplication on GPUs}},
  year      = {2011},
  isbn      = {9781450307710},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2063384.2063392},
  doi       = {10.1145/2063384.2063392},
  booktitle = {{Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis}},
  articleno = {6},
  numpages  = {10},
  keywords  = {matrix-vector multiplication, autotuning, optimization, symmetric matrix, dense linear algebra, pointer redirecting, recursive blocking, GPU},
  location  = {Seattle, Washington},
  series    = {SC '11}
}

@inproceedings{6498575,
  author    = {Deftu, Andrei and Murarasu, Alin},
  booktitle = {{2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing}},
  title     = {{Optimization Techniques for Dimensionally Truncated Sparse Grids on Heterogeneous Systems}},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {351-358},
  doi       = {10.1109/PDP.2013.57}
}

@article{10.1145/3108139,
  author     = {Ashkiani, Saman and Davidson, Andrew and Meyer, Ulrich and Owens, John D.},
  title      = {{GPU Multisplit: An Extended Study of a Parallel Algorithm}},
  year       = {2017},
  issue_date = {March 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {4},
  number     = {1},
  issn       = {2329-4949},
  url        = {https://doi.org/10.1145/3108139},
  doi        = {10.1145/3108139},
  abstract   = {Multisplit is a broadly useful parallel primitive that permutes its input data into contiguous buckets or bins, where the function that categorizes an element into a bucket is provided by the programmer. Due to the lack of an efficient multisplit on Graphics Processing Units (GPUs), programmers often choose to implement multisplit with a sort. One way is to first generate an auxiliary array of bucket IDs and then sort input data based on it. In case smaller indexed buckets possess smaller valued keys, another way for multisplit is to directly sort input data. Both methods are inefficient and require more work than necessary: The former requires more expensive data movements while the latter spends unnecessary effort in sorting elements within each bucket. In this work, we provide a parallel model and multiple implementations for the multisplit problem. Our principal focus is multisplit for a small (up to 256) number of buckets. We use warp-synchronous programming models and emphasize warpwide communications to avoid branch divergence and reduce memory usage. We also hierarchically reorder input elements to achieve better coalescing of global memory accesses. On a GeForce GTX 1080 GPU, we can reach a peak throughput of 18.93Gkeys/s (or 11.68Gpairs/s) for a key-only (or key-value) multisplit. Finally, we demonstrate how multisplit can be used as a building block for radix sort. In our multisplit-based sort implementation, we achieve comparable performance to the fastest GPU sort routines, sorting 32-bit keys (and key-value pairs) with a throughput of 3.0Gkeys/s (and 2.1Gpair/s).},
  journal    = {ACM Trans. Parallel Comput.},
  month      = {aug},
  articleno  = {2},
  numpages   = {44},
  keywords   = {multisplit, radix sort, ballot, shuffle, bucketing, Graphics processing unit (GPU), warp-synchronous programming, histogram}
}

@article{10.1145/2560040,
  author     = {Moussalli, Roger and Salloum, Mariam and Halstead, Robert and Najjar, Walid and Tsotras, Vassilis J.},
  title      = {{A Study on Parallelizing XML Path Filtering Using Accelerators}},
  year       = {2014},
  issue_date = {November 2014},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {13},
  number     = {4},
  issn       = {1539-9087},
  url        = {https://doi.org/10.1145/2560040},
  doi        = {10.1145/2560040},
  journal    = {ACM Trans. Embed. Comput. Syst.},
  month      = {mar},
  articleno  = {93},
  numpages   = {28},
  keywords   = {Publish-subscribe systems, graphics processing units (GPUs), XML, hardware accelerators, field-programmable gate arrays (FPGAs)}
}

@inproceedings{10.1007/978-3-662-53455-7_1,
  author    = {Cruz, Mateus S. and Kozawa, Yusuke and Amagasa, Toshiyuki and Kitagawa, Hiroyuki},
  title     = {{Accelerating Set Similarity Joins Using GPUs}},
  year      = {2016},
  isbn      = {9783662534540},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-662-53455-7_1},
  doi       = {10.1007/978-3-662-53455-7_1},
  booktitle = {{Transactions on Large-Scale Data- and Knowledge-Centered Systems XXVIII - Volume 9940}},
  pages     = {1–22},
  numpages  = {22},
  keywords  = {Similarity join, MinHash, Parallel processing, GPU}
}

@incollection{kernel-def,
  title     = {{Chapter 3 - introduction to data parallelism and CUDA c}},
  booktitle = {{Programming Massively Parallel Processors (2nd Edition)}},
  author    = {D. B. Kirk,  W. mei W. Hwu},
  year      = 2013,
  publisher = {Morgan Kaufmann},
  address   = {Boston}
}

@article{10.1145/3133560,
author = {Leech, Charles and Kumar, Charan and Acharyya, Amit and Yang, Sheng and Merrett, Geoff V. and Al-Hashimi, Bashir M.},
title = {{Runtime Performance and Power Optimization of Parallel Disparity Estimation on Many-Core Platforms}},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/3133560},
doi = {10.1145/3133560},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {nov},
articleno = {41},
numpages = {19},
keywords = {computer vision, power optimization, many-core platforms, Runtime management}
}

@article{10.1145/2611758,
author = {Lim, Jieun and Lakshminarayana, Nagesh B. and Kim, Hyesoon and Song, William and Yalamanchili, Sudhakar and Sung, Wonyong},
title = {{Power Modeling for GPU Architectures Using McPAT}},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/2611758},
doi = {10.1145/2611758},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {jun},
articleno = {26},
numpages = {24},
keywords = {simulation, validation, Design space exploration, Fermi architecture}
}

@article{7152947,
  author  = {Jang, Jae Young and Wang, Hao and Kwon, Euijin and Lee, Jae W. and Kim, Nam Sung},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  title   = {{Workload-Aware Optimal Power Allocation on Single-Chip Heterogeneous Processors}},
  year    = {2016},
  volume  = {27},
  number  = {6},
  pages   = {1838-1851},
  doi     = {10.1109/TPDS.2015.2453965}
}

@inproceedings{10.1145/3037697.3037707,
author = {Park, Jason Jong Kyu and Park, Yongjun and Mahlke, Scott},
title = {{Dynamic Resource Management for Efficient Utilization of Multitasking GPUs}},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3037697.3037707},
doi = {10.1145/3037697.3037707},
abstract = {As graphics processing units (GPUs) are broadly adopted, running multiple applications on a GPU at the same time is beginning to attract wide attention. Recent proposals on multitasking GPUs have focused on either spatial multitasking, which partitions GPU resource at a streaming multiprocessor (SM) granularity, or simultaneous multikernel (SMK), which runs multiple kernels on the same SM. However, multitasking performance varies heavily depending on the resource partitions within each scheme, and the application mixes. In this paper, we propose GPU Maestro that performs dynamic resource management for efficient utilization of multitasking GPUs. GPU Maestro can discover the best performing GPU resource partition exploiting both spatial multitasking and SMK. Furthermore, dynamism within a kernel and interference between the kernels are automatically considered because GPU Maestro finds the best performing partition through direct measurements. Evaluations show that GPU Maestro can improve average system throughput by 20.2% and 13.9% over the baseline spatial multitasking and SMK, respectively.},
booktitle = {{Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
pages = {527–540},
numpages = {14},
keywords = {resource management, graphics processing unit, multitasking},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@article{10.1145/3093337.3037707,
author = {Park, Jason Jong Kyu and Park, Yongjun and Mahlke, Scott},
title = {{Dynamic Resource Management for Efficient Utilization of Multitasking GPUs}},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/3093337.3037707},
doi = {10.1145/3093337.3037707},
abstract = {As graphics processing units (GPUs) are broadly adopted, running multiple applications on a GPU at the same time is beginning to attract wide attention. Recent proposals on multitasking GPUs have focused on either spatial multitasking, which partitions GPU resource at a streaming multiprocessor (SM) granularity, or simultaneous multikernel (SMK), which runs multiple kernels on the same SM. However, multitasking performance varies heavily depending on the resource partitions within each scheme, and the application mixes. In this paper, we propose GPU Maestro that performs dynamic resource management for efficient utilization of multitasking GPUs. GPU Maestro can discover the best performing GPU resource partition exploiting both spatial multitasking and SMK. Furthermore, dynamism within a kernel and interference between the kernels are automatically considered because GPU Maestro finds the best performing partition through direct measurements. Evaluations show that GPU Maestro can improve average system throughput by 20.2% and 13.9% over the baseline spatial multitasking and SMK, respectively.},
journal = {SIGARCH Comput. Archit. News},
month = {apr},
pages = {527–540},
numpages = {14},
keywords = {graphics processing unit, resource management, multitasking}
}

@article{10.1145/3093336.3037707,
author = {Park, Jason Jong Kyu and Park, Yongjun and Mahlke, Scott},
title = {{Dynamic Resource Management for Efficient Utilization of Multitasking GPUs}},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3093336.3037707},
doi = {10.1145/3093336.3037707},
journal = {SIGPLAN Not.},
month = {apr},
pages = {527–540},
numpages = {14},
keywords = {multitasking, graphics processing unit, resource management}
}

@inproceedings{7967160,
  author    = {Tabbakh, Abdulaziz and Annavaram, Murali and Qian, Xuehai},
  booktitle = {{2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}},
  title     = {{Power Efficient Sharing-Aware GPU Data Management}},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {698-707},
  doi       = {10.1109/IPDPS.2017.106}
}

@article{10.1145/2636342,
  author     = {Mittal, Sparsh and Vetter, Jeffrey S.},
  title      = {{A Survey of Methods for Analyzing and Improving GPU Energy Efficiency}},
  year       = {2014},
  issue_date = {January 2015},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {47},
  number     = {2},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/2636342},
  doi        = {10.1145/2636342},
  journal    = {ACM Comput. Surv.},
  month      = {aug},
  articleno  = {19},
  numpages   = {23},
  keywords   = {power model, energy saving, GPU (graphics-processing unit), power management, energy efficiency, architecture techniques, green computing}
}

@article{10.1007/s11227-016-1643-9,
  author     = {Garz\'{o}n, E. M. and Moreno, J. J. and Mart\'{\i}nez, J. A.},
  title      = {{An Approach to Optimise the Energy Efficiency of Iterative Computation on Integrated GPU---CPU Systems}},
  year       = {2017},
  issue_date = {January   2017},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {73},
  number     = {1},
  issn       = {0920-8542},
  url        = {https://doi.org/10.1007/s11227-016-1643-9},
  doi        = {10.1007/s11227-016-1643-9},
  journal    = {J. Supercomput.},
  month      = {jan},
  pages      = {114–125},
  numpages   = {12},
  keywords   = {Parallel iterative algorithms, Heterogeneous processors, Integrated CPU---GPU, Energy efficiency}
}

@misc{concurrent-kernel-execution,
  author = {NVIDIA},
  title  = {{NVIDIA Kepler Architecture Whitepaper}},
  year   = {2012}
}

@ARTICLE{6777559,
  author={Liang, Yun and Huynh, Huynh Phung and Rupnow, Kyle and Goh, Rick Siow Mong and Chen, Deming},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={{Efficient GPU Spatial-Temporal Multitasking}}, 
  year={2015},
  volume={26},
  number={3},
  pages={748-760},
  doi={10.1109/TPDS.2014.2313342}}

@INPROCEEDINGS{7802143,
  author={Mohammadi, Rasoul and Shekofieh, S.- Kazem and Naghibzadeh, Mahmoud and Noori, Hamid},
  booktitle={{2016 6th International Conference on Computer and Knowledge Engineering (ICCKE)}}, 
  title={{A dynamic special-purpose scheduler for concurrent kernels on GPU}}, 
  year={2016},
  volume={},
  number={},
  pages={218-222},
  doi={10.1109/ICCKE.2016.7802143}}

@ARTICLE{6624111,
  author={Zhong, Jianlong and He, Bingsheng},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={{Kernelet: High-Throughput GPU Kernel Executions with Dynamic Slicing and Scheduling}}, 
  year={2014},
  volume={25},
  number={6},
  pages={1522-1532},
  doi={10.1109/TPDS.2013.257}}

@article{10.1145/2499368.2451160,
author = {Pai, Sreepathi and Thazhuthaveetil, Matthew J. and Govindarajan, R.},
title = {{Improving GPGPU Concurrency with Elastic Kernels}},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2499368.2451160},
doi = {10.1145/2499368.2451160},
abstract = {Each new generation of GPUs vastly increases the resources available to GPGPU programs. GPU programming models (like CUDA) were designed to scale to use these resources. However, we find that CUDA programs actually do not scale to utilize all available resources, with over 30% of resources going unused on average for programs of the Parboil2 suite that we used in our work. Current GPUs therefore allow concurrent execution of kernels to improve utilization. In this work, we study concurrent execution of GPU kernels using multiprogram workloads on current NVIDIA Fermi GPUs. On two-program workloads from the Parboil2 benchmark suite we find concurrent execution is often no better than serialized execution. We identify that the lack of control over resource allocation to kernels is a major serialization bottleneck. We propose transformations that convert CUDA kernels into elastic kernels which permit fine-grained control over their resource usage. We then propose several elastic-kernel aware concurrency policies that offer significantly better performance and concurrency compared to the current CUDA policy. We evaluate our proposals on real hardware using multiprogrammed workloads constructed from benchmarks in the Parboil 2 suite. On average, our proposals increase system throughput (STP) by 1.21x and improve the average normalized turnaround time (ANTT) by 3.73x for two-program workloads when compared to the current CUDA concurrency implementation.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {407–418},
numpages = {12},
keywords = {concurrent kernels, gpgpu, cuda}
}

@inproceedings{10.1145/2451116.2451160,
author = {Pai, Sreepathi and Thazhuthaveetil, Matthew J. and Govindarajan, R.},
title = {{Improving GPGPU Concurrency with Elastic Kernels}},
year = {2013},
isbn = {9781450318709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2451116.2451160},
doi = {10.1145/2451116.2451160},
abstract = {Each new generation of GPUs vastly increases the resources available to GPGPU programs. GPU programming models (like CUDA) were designed to scale to use these resources. However, we find that CUDA programs actually do not scale to utilize all available resources, with over 30% of resources going unused on average for programs of the Parboil2 suite that we used in our work. Current GPUs therefore allow concurrent execution of kernels to improve utilization. In this work, we study concurrent execution of GPU kernels using multiprogram workloads on current NVIDIA Fermi GPUs. On two-program workloads from the Parboil2 benchmark suite we find concurrent execution is often no better than serialized execution. We identify that the lack of control over resource allocation to kernels is a major serialization bottleneck. We propose transformations that convert CUDA kernels into elastic kernels which permit fine-grained control over their resource usage. We then propose several elastic-kernel aware concurrency policies that offer significantly better performance and concurrency compared to the current CUDA policy. We evaluate our proposals on real hardware using multiprogrammed workloads constructed from benchmarks in the Parboil 2 suite. On average, our proposals increase system throughput (STP) by 1.21x and improve the average normalized turnaround time (ANTT) by 3.73x for two-program workloads when compared to the current CUDA concurrency implementation.},
booktitle = {{Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
pages = {407–418},
numpages = {12},
keywords = {cuda, gpgpu, concurrent kernels},
location = {Houston, Texas, USA},
series = {ASPLOS '13}
}

@inproceedings{10.1145/2751205.2751213,
author = {Wu, Bo and Chen, Guoyang and Li, Dong and Shen, Xipeng and Vetter, Jeffrey},
title = {{Enabling and Exploiting Flexible Task Assignment on GPU through SM-Centric Program Transformations}},
year = {2015},
isbn = {9781450335591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751205.2751213},
doi = {10.1145/2751205.2751213},
abstract = {A GPU's computing power lies in its abundant memory bandwidth and massive parallelism. However, its hardware thread schedulers, despite being able to quickly distribute computation to processors, often fail to capitalize on program characteristics effectively, achieving only a fraction of the GPU's full potential. Moreover, current GPUs do not allow programmers or compilers to control this thread scheduling, forfeiting important optimization opportunities at the program level. This paper presents a transformation centered on Streaming Multiprocessors (SM); this software approach to circumventing the limitations of the hardware scheduler allows flexible program-level control of scheduling. By permitting precise control of job locality on SMs, the transformation overcomes inherent limitations in prior methods.With this technique, flexible control of GPU scheduling at the program level becomes feasible, which opens up new opportunities for GPU program optimizations. The second part of the paper explores how the new opportunities could be leveraged for GPU performance enhancement, what complexities there are, and how to address them. We show that some simple optimization techniques can enhance co-runs of multiple kernels and improve data locality of irregular applications, producing 20-33% average increase in performance, system throughput, and average turnaround time.},
booktitle = {{Proceedings of the 29th ACM on International Conference on Supercomputing}},
pages = {119–130},
numpages = {12},
keywords = {compiler transformation, gpgpu, scheduling, program co-run, data affinity},
location = {Newport Beach, California, USA},
series = {ICS '15}
}

@INPROCEEDINGS{7446078,
  author={Wang, Zhenning and Yang, Jun and Melhem, Rami and Childers, Bruce and Zhang, Youtao and Guo, Minyi},
  booktitle={{2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)}}, 
  title={{Simultaneous Multikernel GPU: Multi-tasking throughput processors via fine-grained sharing}}, 
  year={2016},
  volume={},
  number={},
  pages={358-369},
  doi={10.1109/HPCA.2016.7446078}}

@article{10.1145/3007787.3001161,
author = {Xu, Qiumin and Jeon, Hyeran and Kim, Keunsoo and Ro, Won Woo and Annavaram, Murali},
title = {{Warped-Slicer: Efficient Intra-SM Slicing through Dynamic Resource Partitioning for GPU Multiprogramming}},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/3007787.3001161},
doi = {10.1145/3007787.3001161},
abstract = {As technology scales, GPUs are forecasted to incorporate an ever-increasing amount of computing resources to support thread-level parallelism. But even with the best effort, exposing massive thread-level parallelism from a single GPU kernel, particularly from general purpose applications, is going to be a difficult challenge. In some cases, even if there is sufficient thread-level parallelism in a kernel, there may not be enough available memory bandwidth to support such massive concurrent thread execution. Hence, GPU resources may be underutilized as more general purpose applications are ported to execute on GPUs. In this paper, we explore multiprogramming GPUs as a way to resolve the resource underutilization issue. There is a growing hardware support for multiprogramming on GPUs. Hyper-Q has been introduced in the Kepler architecture which enables multiple kernels to be invoked via tens of hardware queue streams. Spatial multitasking has been proposed to partition GPU resources across multiple kernels. But the partitioning is done at the coarse granularity of streaming multiprocessors (SMs) where each kernel is assigned to a subset of SMs. In this paper, we advocate for partitioning a single SM across multiple kernels, which we term as intra-SM slicing. We explore various intra-SM slicing strategies that slice resources within each SM to concurrently run multiple kernels on the SM. Our results show that there is not one intra-SM slicing strategy that derives the best performance for all application pairs. We propose Warped-Slicer, a dynamic intra-SM slicing strategy that uses an analytical method for calculating the SM resource partitioning across different kernels that maximizes performance. The model relies on a set of short online profile runs to determine how each kernel's performance varies as more thread blocks from each kernel are assigned to an SM. The model takes into account the interference effect of shared resource usage across multiple kernels. The model is also computationally efficient and can determine the resource partitioning quickly to enable dynamic decision making as new kernels enter the system. We demonstrate that the proposed Warped-Slicer approach improves performance by 23% over the baseline multiprogramming approach with minimal hardware overhead.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {230–242},
numpages = {13},
keywords = {resource management, multiprogramming, multi-kernel, GPUs, scheduling}
}

@inproceedings{10.1109/ISCA.2016.29,
author = {Xu, Qiumin and Jeon, Hyeran and Kim, Keunsoo and Ro, Won Woo and Annavaram, Murali},
title = {{Warped-Slicer: Efficient Intra-SM Slicing through Dynamic Resource Partitioning for GPU Multiprogramming}},
year = {2016},
isbn = {9781467389471},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2016.29},
doi = {10.1109/ISCA.2016.29},
abstract = {As technology scales, GPUs are forecasted to incorporate an ever-increasing amount of computing resources to support thread-level parallelism. But even with the best effort, exposing massive thread-level parallelism from a single GPU kernel, particularly from general purpose applications, is going to be a difficult challenge. In some cases, even if there is sufficient thread-level parallelism in a kernel, there may not be enough available memory bandwidth to support such massive concurrent thread execution. Hence, GPU resources may be underutilized as more general purpose applications are ported to execute on GPUs. In this paper, we explore multiprogramming GPUs as a way to resolve the resource underutilization issue. There is a growing hardware support for multiprogramming on GPUs. Hyper-Q has been introduced in the Kepler architecture which enables multiple kernels to be invoked via tens of hardware queue streams. Spatial multitasking has been proposed to partition GPU resources across multiple kernels. But the partitioning is done at the coarse granularity of streaming multiprocessors (SMs) where each kernel is assigned to a subset of SMs. In this paper, we advocate for partitioning a single SM across multiple kernels, which we term as intra-SM slicing. We explore various intra-SM slicing strategies that slice resources within each SM to concurrently run multiple kernels on the SM. Our results show that there is not one intra-SM slicing strategy that derives the best performance for all application pairs. We propose Warped-Slicer, a dynamic intra-SM slicing strategy that uses an analytical method for calculating the SM resource partitioning across different kernels that maximizes performance. The model relies on a set of short online profile runs to determine how each kernel's performance varies as more thread blocks from each kernel are assigned to an SM. The model takes into account the interference effect of shared resource usage across multiple kernels. The model is also computationally efficient and can determine the resource partitioning quickly to enable dynamic decision making as new kernels enter the system. We demonstrate that the proposed Warped-Slicer approach improves performance by 23% over the baseline multiprogramming approach with minimal hardware overhead.},
booktitle = {{Proceedings of the 43rd International Symposium on Computer Architecture}},
pages = {230–242},
numpages = {13},
keywords = {GPUs, resource management, scheduling, multiprogramming, multi-kernel},
location = {Seoul, Republic of Korea},
series = {ISCA '16}
}

@INPROCEEDINGS{8327010,
  author={Dai, Hongwen and Lin, Zhen and Li, Chao and Zhao, Chen and Wang, Fei and Zheng, Nanning and Zhou, Huiyang},
  booktitle={{2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)}}, 
  title={{Accelerate GPU Concurrent Kernel Execution by Mitigating Memory Pipeline Stalls}}, 
  year={2018},
  volume={},
  number={},
  pages={208-220},
  doi={10.1109/HPCA.2018.00027}}

@article{8219713,
  author  = {Reaño, Carlos and Silla, Federico and Nikolopoulos, Dimitrios S. and Varghese, Blesson},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  title   = {{Intra-Node Memory Safe GPU Co-Scheduling}},
  year    = {2018},
  volume  = {29},
  number  = {5},
  pages   = {1089-1102},
  doi     = {10.1109/TPDS.2017.2784428}
}

@inproceedings{8310677,
  author    = {Kiani, Mohsen and Rajabzadeh, Amir},
  booktitle = {{2017 19th International Symposium on Computer Architecture and Digital Systems (CADS)}},
  title     = {{SKERD: Reuse distance analysis for simultaneous multiple GPU kernel executions}},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {1-6},
  doi       = {10.1109/CADS.2017.8310677}
}
